{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guyle\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO, YOLOv10\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from boxmot import DeepOCSORT\n",
    "from tqdm import tqdm\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as con "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions\n",
    "colors = [(0,170,170),(250,110,25),(40,40,40),(255,90,210),(65,150,60),(100,100,100),(110,60,60),(200,200,200),(110,110,170),(240,240,50)]\n",
    "def sizefunction(conf:float):\n",
    "    if(conf < 0.5):\n",
    "        return 2\n",
    "    elif(conf < 0.6):\n",
    "        return 3\n",
    "    elif(conf < 0.7):\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "def TrackerProcess(im, results, tracker,false_width,false_height):\n",
    "    width, height = (0,0)\n",
    "    if(type(im) == np.ndarray):\n",
    "        height, width , _ = im.shape\n",
    "    else:\n",
    "        width, height = im.size\n",
    "        \n",
    "    objects = np.zeros((len(results[0].boxes.conf),6))\n",
    "    j = 0\n",
    "    for box, conf, cls in zip(results[0].boxes.xyxy,results[0].boxes.conf,results[0].boxes.cls):\n",
    "        box = box.numpy().astype(int).tolist()\n",
    "        for k in range(4):\n",
    "            objects[j][k] = box[k]\n",
    "        objects[j][5] = int(cls.item())\n",
    "        objects[j][4] = conf.item()\n",
    "        j += 1\n",
    "    #print(objects)\n",
    "    tracker.update(objects, im) # --> M X (x, y, x, y, id, conf, cls, ind)\n",
    "    for a in tracker.active_tracks:\n",
    "        if(a.history_observations and len(a.history_observations) > 2):\n",
    "            x1, y1, x2, y2, conf = a.history_observations[-1]\n",
    "            cls = a.cls\n",
    "            r,g,b = colors[int(cls.item())]\n",
    "            cv2.rectangle(im, (int(x1 * width/false_width), int(y1 * height/false_height)), (int(x2* width/false_width), int(y2 * height/false_height)), (b,g,r), sizefunction(conf))\n",
    "    #print(tracker.active_tracks.history_observations[-1])\n",
    "    # for x1,y1,x2,y2,conf,cls in tracker_results:\n",
    "    #     conf = conf.item()\n",
    "    #     cv2.rectangle(im, (int(x1), int(y1)), (int(x2), int(y2)), colors[int(cls.item())], sizefunction(conf))\n",
    "    return im\n",
    "def NoTrackerProcess(im,results):\n",
    "    width, height = (0,0)\n",
    "    if(type(im) == np.ndarray):\n",
    "        height, width , _ = im.shape\n",
    "    else:\n",
    "        width, height = im.size\n",
    "\n",
    "    for (x1,y1,x2,y2),conf,cls in zip(results[0].boxes.xyxyn,results[0].boxes.conf,results[0].boxes.cls):\n",
    "        #print((x1,y1,x2,y2), width,height, (int(x1 * width), int(y1 * height)), (int(x2 * width), int(y2 * height)))\n",
    "        conf = conf.item()\n",
    "        r,g,b = colors[int(cls.item())]\n",
    "        cv2.rectangle(im, (int(x1 * width), int(y1 * height)), (int(x2 * width), int(y2 * height)), (b,g,r), sizefunction(conf))\n",
    "    return im\n",
    "def PutText2(im, text:str):\n",
    "    height, width, layers = im.shape\n",
    "    org = (int(width/3), height-20)  # Bottom-left corner of the text string in the image\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 7\n",
    "    color = (0, 0, 0)  # Blue color in BGR\n",
    "    thickness = 4\n",
    "    line_type = cv2.LINE_AA\n",
    "    cv2.putText(im, text, org, font, font_scale, color, thickness, line_type)\n",
    "    return im\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((640, 640)),  # Resize to the model's input shape\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "def GaussianUnsharp2(image):\n",
    "    gaussian_3 = cv2.GaussianBlur(image, (0, 0), 2.0)\n",
    "    unsharp_image = cv2.addWeighted(image, 2.0, gaussian_3, -1.0, 0)\n",
    "    return unsharp_image\n",
    "def Preprocess(img):\n",
    "    color_image = np.array(img)\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    ycrcb_img = cv2.cvtColor(color_image, cv2.COLOR_RGB2YCrCb)\n",
    "    ycrcb_img[:, :, 0] = clahe.apply(ycrcb_img[:, :, 0])#cv2.equalizeHist(ycrcb_img[:, :, 0])\n",
    "    equalized_color_image = cv2.cvtColor(ycrcb_img, cv2.COLOR_YCrCb2RGB)\n",
    "    return transform(Image.fromarray(GaussianUnsharp2(equalized_color_image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2024-07-04 10:20:10.148\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.utils.torch_utils\u001b[0m:\u001b[36mselect_device\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mYolo Tracking v10.0.71 üöÄ Python-3.10.6 torch-2.3.0+cu121\n",
      "CUDA:0 (NVIDIA GeForce RTX 4070 SUPER, 12282MiB)\u001b[0m\n",
      "\u001b[32m2024-07-04 10:20:10.216\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid_model_factory\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m207\u001b[0m - \u001b[32m\u001b[1mSuccessfully loaded pretrained weights from \"osnet_x0_25_dukemtmcreid.pt\"\u001b[0m\n",
      " 25%|‚ñà‚ñà‚ñç       | 740/3000 [06:02<18:28,  2.04it/s]\n"
     ]
    }
   ],
   "source": [
    "#Process each frame, label it using the pretrained YOLO network and output a video\n",
    "tracker = DeepOCSORT(\n",
    "    model_weights=Path(\"osnet_x0_25_dukemtmcreid.pt\"),#Path('osnet_x0_25_msmt17.pt'), # which ReID model to use\n",
    "    device='cuda:0',\n",
    "    fp16=True,\n",
    "    max_age=3\n",
    ")\n",
    "vid = cv2.VideoCapture(VideoPath)\n",
    "model = YOLO(YOLOPath).cpu()\n",
    "frames = range(0,3000)\n",
    "#frames = range(0,1)\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'XVID' or 'MJPG' for .avi files\n",
    "output_video = None\n",
    "UsingTracker = True\n",
    "Tandem = False\n",
    "vid.set(cv2.CAP_PROP_POS_FRAMES, min(frames))\n",
    "\n",
    "for i in tqdm(frames):\n",
    "    ret, im = vid.read()\n",
    "    \n",
    "    if(ret):\n",
    "        im_network = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "        im_network = Preprocess(Image.fromarray(im_network)).unsqueeze(0)\n",
    "        _,actual_width, actual_height, _ = im_network.shape\n",
    "        height, width, layers = im.shape\n",
    "        if(output_video == None):\n",
    "            output_video = cv2.VideoWriter(OutputPath, fourcc, 10, (2 * width if Tandem else width, height),isColor=True)\n",
    "        results = model(im_network,conf = 0.1, verbose=False)\n",
    "        if(not Tandem):\n",
    "            if(UsingTracker):\n",
    "                TrackerProcess(im,results,tracker,actual_width, actual_height)\n",
    "            else:\n",
    "                NoTrackerProcess(im,results)\n",
    "        else:\n",
    "            im1 = im.copy()\n",
    "            im2 = im.copy()\n",
    "            \n",
    "            YCrCb(im1,results,tracker, actual_width, actual_height)\n",
    "            NoTrackerProcess(im2,results)\n",
    "            im1 = PutText2(im1, \"DeepOCSORT\")\n",
    "            im2 = PutText2(im2, \"No Tracker\")\n",
    "            im = np.hstack((im1, im2))\n",
    "            #print(im.shape, im1.shape)\n",
    "            #plt.imshow(im)\n",
    "        output_video.write(im)\n",
    "    elif(not ret):\n",
    "        break\n",
    "vid.release()\n",
    "output_video.release()\n",
    "cv2.destroyAllWindows()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|‚ñã         | 740/10000 [02:32<31:45,  4.86it/s]\n"
     ]
    }
   ],
   "source": [
    "#create labels file \n",
    "\n",
    "\n",
    "vid = cv2.VideoCapture(VideoPath)\n",
    "model = YOLO(YOLOPath).cuda()\n",
    "frames = range(0,10000)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'XVID' or 'MJPG' for .avi files\n",
    "with open(OutputPath,'w') as f:\n",
    "    for i in tqdm(frames):\n",
    "        ret, im = vid.read()\n",
    "        \n",
    "        if(ret):\n",
    "            im_network = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "            im_network = Preprocess(Image.fromarray(im_network))\n",
    "            results = model(im_network.unsqueeze(0).cuda(),conf = 0.2, verbose=False)\n",
    "            for (x,y,w,h),conf,cls in zip(results[0].boxes.xywhn,results[0].boxes.conf,results[0].boxes.cls):\n",
    "                f.write(f\"{int(cls)} {x:.4f} {y:.4f} {w:.4f} {h:.4f}\\n\")\n",
    "            f.write('\\n')\n",
    "        elif(not ret):\n",
    "            break\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
